# Bias variance debate

### The bias-variance debate  <a id="sec:biasVariance"></a>

Assume there is a set of given instances, $$X$$ \(m rows and n columns\), and we are asked to optimize a model $$M(X, \theta)$$ to estimate $$Y$$ \($$m$$ rows\) given some examples, i.e., supervised learning. Also, it is assumed that the underlying pattern of the data is not known \(note that, this is a very important assumption as if this is not true then any model except the one which formulates the pattern is irrelevant\). Another assumption is that the collected data may have some noise in it \(another important assumption\). If $M$ is a flexible formula \(which usually means it involves lots of parameters to complex formulations\) then it can fit to the given data-set very well. This, however, means that the model would formulate also the noise in the data, which would impose lots of fluctuations and non-linearities that are not really related to the pattern in the data but the noise in the observations. This leads to a model that has a low bias. However, as it is very flexible and formulates lots of fluctuations coming from random events \(noise\), it also offer a high variance. This high variance and low bias would lead to worsening the generalization ability of the model as it is not really formulating the pattern in the data but also the noise and unnecessary fluctuations.

Let's go a bit more detail. Let us call each instance $$x$$ and its corresponding desired output as $$y$$. In reality, $$y$$ is the outcome of a system working with inputs $$x$$, and some noise, $$y=f(x)+\epsilon$$, where $$f(.)$$ is not known and $$\epsilon$$ is noise which is also unknown. For example, if $$x$$ represents characteristics of people \(smoking habit, weight, genetics, etc.\) and $$y$$ is whether or not they would have a heart-attack after their 50s, then $$f(x)$$ is how body would respond to those characteristics and leads to a heart-attack or not, which is not really known. With modelling, we estimate this function $$f(x)$$, given some examples and some assumptions on the shape of $$f$$.

A model $$M$$ is responsible to estimate observed $$Y$$ as $$Y=f(X)+\epsilon=M(X, \hat{\theta})+e$$, where $$e$$ has $$m$$ rows and indicates the error from $$Y$$. If $$e$$ is small then $$M(X, \hat{\theta})$$ is an accurate estimation of $$Y$$, hence the model estimates not only $$f$$ but large amount of the noise $$\epsilon$$ in it. This takes place if the model $$M$$ is a complex, flexible, equation which is able to fit any complex behavior of $$Y$$ as function of $$X$$, including all fluctuations resulting from the noise. Hence, the outputs of the model will be similar to the values of $$Y$$ \(non-biased\), which includes noisy fluctuations, which leads to a high variance \(fluctuations usually lead to large variance\). If the model is not complex though, the gap between the output of the model and $$Y$$ might be large \(bias\), however, it would fit less to the noisy fluctuations in the $$Y$$ which leads to a smaller variance. That is why it is said a complex model has a large variance and small bias, and vice versa. Formal calculation of this trade off can be found in the [bias-variance tradeoff in wikipedia](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#Derivation).

