# Features vs raw data

The data in its original form, raw form, represents what is measured. The measurements, however, are not always useful to learn from. For example, in a stock market signal, the actual close daily prices are not as important as the trend of those values. Hence, depending on the problem, some characteristics of the data might be more informative than the raw data itself. These characteristics are usually called features. Features basically represent important \(relevant to a given problem which uses that data for solution\) attributes of the problem. They are either designed by an expert \(i.e., feature engineering\) or optimized by an algorithm for a given model \(i.e., feature formation, popular in deep learning\). For example, assume that we want to characterize fruits. Some obvious features of fruits are their shapes \(e.g., being round, or egg-shaped, or long\), their tastes, and their smell. Features characterize attributes that distinguish different objects

For many problems, the features are already known. For example, we already know that a feed in a newspaper is considered interesting for a particular reader if the reader spends more time on it \(the spent time is a feature\). However, recognizing someone's face from an image is not easy to be defined by apparent features. This encourages methods like Deep Learning to not only learn the task but also find the best presentation for the problem, just from the raw data.

\#\#TOCOMPLETE

Talk about Bagging as well

