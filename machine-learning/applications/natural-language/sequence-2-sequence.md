# Sequence 2 sequence

## Introduction

Sequence-to-sequence learning \(Seq2Seq\) is about training models to convert sequences from one domain \(e.g. sentences in English\) to sequences in another domain \(e.g. the same sentences translated to French\).

```text
"the cat sat on the mat" -> [Seq2Seq model] -> "le chat etait assis sur le tapis"
```

Using RNN/LSTM

Using Convnet1D

## Seq2seq using RNN

### How it works



![](../../../.gitbook/assets/image%20%2814%29.png)







![Source &#x2013; https://github.com/google/seq2seq](../../../.gitbook/assets/image%20%2813%29.png)





### Implementation

* See [this link](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html) for an implementation in Keras





